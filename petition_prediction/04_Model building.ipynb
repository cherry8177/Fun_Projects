{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('Vic_petition_data.csv',encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = (df[['Title', 'Text','Title_str_len' ,\n",
    "       'start_time', 'Text_len_p1','Text_len_p2', 'Title_len', 'Text_str_len','Image','Tweet']], \n",
    "         df.No_Supporters_log)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_court_vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,\n",
    "                             ngram_range=(1, 2),\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_text_features = text_court_vectorizer.fit_transform((x_train.Text).values.astype('U'))\n",
    "train_text_features = train_text_features.toarray()\n",
    "test_text_features = text_court_vectorizer.transform((x_test.Text).values.astype('U'))\n",
    "test_text_features = test_text_features.toarray()\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump(text_court_vectorizer,open('text_court_vectorizer_v2.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_court_vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                             tokenizer = None,\n",
    "                             ngram_range=(1, 3),\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None,   \n",
    "                             max_features = 2500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_title_features = title_court_vectorizer.fit_transform((x_train.Title).values.astype('U'))\n",
    "train_title_features = train_title_features.toarray()\n",
    "test_title_features = title_court_vectorizer.transform((x_test.Title).values.astype('U'))\n",
    "test_title_features = test_title_features.toarray()\n",
    "\n",
    "#import pickle\n",
    "pickle.dump(title_court_vectorizer,open('title_court_vectorizer_v2.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#model_tSNE_all = TSNE(n_components=2, random_state=0)\n",
    "total_scale_train=StandardScaler().fit(np.concatenate((train_text_features,\n",
    "                                                     train_title_features,x_train[['Title_str_len', 'Text_len_p1', 'Text_len_p2',\n",
    "                                      'Title_len', 'Text_str_len','Image','Tweet']]),axis=1))\n",
    "\n",
    "total_train_scale=total_scale_train.transform(np.concatenate((train_text_features,\n",
    "                                                     train_title_features,x_train[[\n",
    "                                                         'Title_str_len', 'Text_len_p1',\n",
    "                                                         'Text_len_p2',\n",
    "                                      'Title_len', 'Text_str_len','Image','Tweet']]),axis=1))\n",
    "\n",
    "\n",
    "total_test_scale=total_scale_train.transform(np.concatenate((test_text_features,\n",
    "                                                             test_title_features, \n",
    "                                                x_test[['Title_str_len', 'Text_len_p1',\n",
    "                                                        'Text_len_p2',\n",
    "                                      'Title_len', 'Text_str_len','Image','Tweet']]),axis=1))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "train_pca=PCA(n_components=1500,svd_solver='full')\n",
    "\n",
    "x_train_pca=train_pca.fit_transform(total_train_scale)\n",
    "\n",
    "x_test_pca=train_pca.transform(total_test_scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(total_scale_train,open('total_scale_train_v2.pickle','wb'))\n",
    "pickle.dump(train_pca,open('train_pca.pickle_v2','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Elastic = linear_model.ElasticNet()\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[\n",
    "    ('pca', pca), \n",
    "                       ('Elastic', Elastic)])\n",
    "\n",
    "n_components = [ 60,100,500,750, 1000,]\n",
    "alphas = np.logspace(-4, 4, 4)\n",
    "l1_ratio=( 0.2,0.4,0.6,0.7,0.8,1)\n",
    "\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              Elastic__alpha=alphas,Elastic__l1_ratio=l1_ratio))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('Elastic', ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'pca__n_components': [60, 100, 500, 750, 1000], 'Elastic__alpha': array([  1.00000e-04,   4.64159e-02,   2.15443e+01,   1.00000e+04]), 'Elastic__l1_ratio': (0.2, 0.4, 0.6, 0.7, 0.8, 1)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(x_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.0464158883361 0.6\n"
     ]
    }
   ],
   "source": [
    "n_comp=estimator.best_estimator_.named_steps['pca'].n_components\n",
    "alpha=estimator.best_estimator_.named_steps['Elastic'].alpha\n",
    "l1=estimator.best_estimator_.named_steps['Elastic'].l1_ratio\n",
    "\n",
    "\n",
    "print(n_comp,alpha,l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Elastic=linear_model.ElasticNet(alpha=alpha, \n",
    "                    l1_ratio=l1, fit_intercept=True, \n",
    "            normalize=False, precompute=False, max_iter=1000, \n",
    "           copy_X=True, tol=0.0001, warm_start=False, positive=False, \n",
    "           random_state=None, selection='cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Elastic=Elastic.fit(x_train_pca[:,0:n_comp],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(train_Elastic,open('train_Elastic_v2.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predict=train_Elastic.predict(x_train_pca[:,0:n_comp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57806138559572984"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_train=mean_squared_error(y_train, train_predict);MSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predict=train_Elastic.predict(x_test_pca[:,0:n_comp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8859208285978879"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE_test=mean_squared_error(y_test, test_predict); MSE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22412212700140033"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.86483976102 3.81526681852\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(test_predict), np.mean(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
