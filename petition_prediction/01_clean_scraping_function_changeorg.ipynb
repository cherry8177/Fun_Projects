{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dateutil\n",
    "import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import re\n",
    "import html5lib\n",
    "import html\n",
    "import pickle as pk\n",
    "from fake_useragent import UserAgent\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# timing function\n",
    "def timefunc(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print f.__name__, 'took', end - start, 'seconds'\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Recent Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/feiwang/Documents/python_code/scraping\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up chromedriver\n",
    "chromedriver = '/Users/feiwang/Documents/python_code/scraping/chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver\n",
    "#chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go straight to most-recent pages\n",
    "petitions_url = 'https://www.change.org/petitions#most-recent/' # up to 5084\n",
    "\n",
    "@timefunc\n",
    "def get_petition_urls(base, start, stop):\n",
    "    d = {}\n",
    "    for i in range(start, stop+1):\n",
    "        driver.get(base + str(i))\n",
    "        petitions = driver.find_elements_by_xpath('//div[@class = \"petition-list\"]//ol//li[@class = \"petition\"]')\n",
    "        for p in petitions:\n",
    "            d[p.get_attribute('data-id')] = p.get_attribute('data-url')\n",
    "    return d\n",
    "\n",
    "@timefunc\n",
    "def chunk_petition_urls(dic, base, first, last, chunk):\n",
    "    for c in range(first, last+1, chunk):\n",
    "        d = get_petition_urls(base, c, c+chunk)\n",
    "        dic.update(d)\n",
    "        filename = 'master' + str(c+chunk) + '.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pk.dump(dic,f,-1)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master = {}\n",
    "urls1000 = chunk_petition_urls(master, petitions_url, 1, 1000, 100)\n",
    "pk.dump(master, open('master_recent_url.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get victorial url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chromedriver = '/Users/feiwang/Documents/python_code/Petition Prediction/chromedriver'\n",
    "os.environ['webdriver.chrome.driver'] = chromedriver\n",
    "#chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "victories_url = 'https://www.change.org/victories#most-recent/' \n",
    "\n",
    "@timefunc\n",
    "def get_victory_urls(base, start, stop):\n",
    "    d = {}\n",
    "    for i in range(start, stop+1):\n",
    "        driver.get(base + str(i))\n",
    "        sleep(random.randint(1,3))\n",
    "        victories = driver.find_elements_by_xpath('//div[@class = \"petition-list\"]//ol//li[@class = \"petition\"]')\n",
    "        for i, v in enumerate(victories):\n",
    "            d[i] = v.get_attribute('data-url')\n",
    "    return d\n",
    "\n",
    "@timefunc\n",
    "def chunk_victory_urls(dic, base, first, last, chunk):\n",
    "    for c in range(first, last+1, chunk):\n",
    "        d = get_victory_urls(base, c, c+chunk)\n",
    "        dic.update(d)\n",
    "        filename = 'victories' + str(c+chunk) + '.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pk.dump(dic,f,-1)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "victories = {}\n",
    "victories = get_victory_urls(victories_url, 1, 200)\n",
    "filename = 'victories_final.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pk.dump(victories,f,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_petition_data_recent(user_agent,url ):\n",
    "    req = urllib2.Request(url, headers={'User-Agent' : user_agents})\n",
    "    print url\n",
    "    con = urllib2.urlopen( req )\n",
    "    html = con.read()\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    ###########################extract title\n",
    "    titles=''\n",
    "    titles = soup.findAll(\"div\", { \"class\" : \"txt-c\" })\n",
    "    title=[]\n",
    "    for element in titles:\n",
    "        if element.select('h1')!=[]:\n",
    "             title=element.select('h1')           \n",
    "    title=re.sub('(<.*?>)',\" \",str(title[0]))\n",
    "    print title \n",
    "\n",
    "    ########################extract text\n",
    "    texts=\"\"\n",
    "    texts=soup.findAll(\"div\", { \"class\" : \"rte js-description-content\" })\n",
    "    print texts\n",
    "\n",
    "    ########################extract the supporter number\n",
    "    Nsupporter_fling=''\n",
    "    Nsupporter_fling=soup.findAll(\"div\", { \"class\" : \"col-xs-4 type-s js-mobile-supporter-count\" })\n",
    "\n",
    "    ugly_Nsupporter=Nsupporter_fling[0].select('strong')[0]\n",
    "    N_supporters=re.sub('(<.*?>)',\" \",str(ugly_Nsupporter))                       \n",
    "    print N_supporters\n",
    "\n",
    "    one_petition_info={url:[title,texts, N_supporters] }\n",
    "\n",
    "    return one_petition_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recent_petition_dict={}\n",
    "ua = UserAgent()\n",
    "user_agents=[ua.ie, ua.msie,\n",
    "              ua['Internet Explorer'], ua.opera,\n",
    "              ua.chrome,ua.google,\n",
    "              ua['google chrome'],\n",
    "              ua.firefox,ua.ff,\n",
    "              ua.safari,ua.random]\n",
    "\n",
    "with open('master_recent_url.pickle','rb') as f:\n",
    "    url_chuck=pk.load(f)\n",
    "#petition_chuck={}\n",
    "for url in url_chuck.values():\n",
    "    one_petition=get_petition_data_recent(user_agents,str(url))\n",
    "    sleep(random.randint(1,10))\n",
    "    #petition_chuck.update(one_petition)\n",
    "    recent_petition_dict.update(one_petition)\n",
    "#pk.dump(petition_chunck,open('recent_petition_chunk'+str(i)+'.pkl','wb'))\n",
    "#pk.dump(recent_petition_dict,open('recent_petition.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_recent_dict={}\n",
    "remove_fish=lambda x: re.sub('(<.*?>)',\" \",x)\n",
    "for key in recent_petition_dict:\n",
    "    clean_array=[]\n",
    "    for entry in recent_petition_dict[key][1]:\n",
    "        if entry.select('p'):\n",
    "            clean_array.append(entry.select('p'))\n",
    "        else:\n",
    "            clean_array=['NA']\n",
    "        texts=[]\n",
    "        if len(clean_array):\n",
    "            for element in clean_array:\n",
    "                soup=BeautifulSoup(str(element)).text\n",
    "                texts.append(re.sub('(<p>)',\" \",str(soup)))\n",
    "    clean_recent_dict[key]=[ recent_petition_dict[key][0],texts,recent_petition_dict[key][2]]\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
